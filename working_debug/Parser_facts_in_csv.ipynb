{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "table.dataframe td, table.dataframe th {\n",
       "    border: 1px  black solid !important;\n",
       "  color: black !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2': '1'}\n",
      "{}\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "##CREATION OF PYTHON DICTS and LISTS FROM NECESSARY FILES\n",
    "import sys, os, pandas as pd, numpy as np, itertools, re\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#Specify path of sentence:\n",
    "tmp_path=os.getenv('HOME_anu_tmp')+'/tmp/'\n",
    "eng_file_name = 'BUgol2.2E'\n",
    "# eng_file_name = 'BUgol2.2E'\n",
    "\n",
    "sent_no = '2.82' #2.29, 2.21, 2.61, 2.14, 2.64\n",
    "path_tmp= tmp_path + eng_file_name + \"_tmp/\" + sent_no\n",
    "filename =path_tmp +  '/H_wordid-word_mapping.dat'\n",
    "efilename = path_tmp + '/E_wordid-word_mapping.dat'\n",
    "\n",
    "\n",
    "roja_transliterate_file = path_tmp +  '/results_of_transliteration.dat'\n",
    "nandani_file = path_tmp +  '/corpus_specific_dic_facts_for_one_sent.dat'\n",
    "\n",
    "data=\"\"; tranliterate_dict={}\n",
    "def extract_dictionary_from_deftemplate(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = f.read().split(\"\\n\")\n",
    "#         print(data)\n",
    "        while \"\" in data:\n",
    "            data.remove(\"\")\n",
    "            \n",
    "        tranliterate_dict={}\n",
    "\n",
    "        for line in data:\n",
    "#             print(line)\n",
    "            key = line.split(\")\")[0].lstrip(\"Edict-Hdict (E_id \")\n",
    "            val = line.split(\")\")[2].lstrip(\"(H_id \")\n",
    "#             print(key, val)\n",
    "            tranliterate_dict[key]=val\n",
    "    return(tranliterate_dict)\n",
    "\n",
    "try:\n",
    "    transliterate_mapping = extract_dictionary_from_deftemplate(roja_transliterate_file)  \n",
    "    print(transliterate_mapping)\n",
    "\n",
    "except:\n",
    "    print(\"FILE MISSING: \" + roja_transliterate_file )\n",
    "nandani_mapping = extract_dictionary_from_deftemplate(nandani_file)  \n",
    "print(nandani_mapping)\n",
    "# print(data)\n",
    "\n",
    "\n",
    "#Function to extract dictionary from H_wordid-word_mapping.dat\n",
    "def parser2wordid1(filename):\n",
    "    with open(filename,\"r\") as f1: \n",
    "        text = f1.read().split(\"\\n\")\n",
    "        while(\"\" in text) :\n",
    "            text.remove(\"\")\n",
    "        p2w = {}\n",
    "        for line in text:\n",
    "            t = line.lstrip('(H_wordid-word').strip(')').split(\"\\t\")\n",
    "            p2w[int(t[1].lstrip(\"P\"))] = t[2]\n",
    "    return(p2w)\n",
    "\n",
    "#Function to extract dictionary from E_wordid-word_mapping.dat\n",
    "def parser2wordid(filename):\n",
    "    with open(filename,\"r\") as f1: \n",
    "        text = f1.read().split(\"\\n\")\n",
    "        while(\"\" in text) :\n",
    "            text.remove(\"\")\n",
    "        p2w = {}\n",
    "        for line in text:\n",
    "            t = line.lstrip('(E_wordid-word').strip(')').split(\"\\t\")\n",
    "            p2w[int(t[1].lstrip(\"P\"))] = t[2]\n",
    "    return(p2w)\n",
    "    \n",
    "p2w = parser2wordid1(filename)\n",
    "e2w = parser2wordid(efilename)\n",
    "# print(p2w)\n",
    "# print(e2w)\n",
    "\n",
    "\n",
    "# extracting df from BUgol2.1E_2.21_1.csv which contains  old dictionary facts with hindi word is and not parser id   \n",
    "dfs = pd.read_csv(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_1.csv\")\n",
    "\n",
    "#row index started from 1 instead of 0, which was earlier.\n",
    "dfs.index = np.arange(1,len(dfs)+1)\n",
    "\n",
    "# print(dfs.shape)\n",
    "# r = len(p2w)\n",
    "# c = dfs.shape[1] - 1\n",
    "# print(r, c)\n",
    "# r_list = range(1,r+1)\n",
    "# c_list =range(1,c+1)\n",
    "# print(r_list, c_list)\n",
    "\n",
    "# df = pd.Dataframe(rows=r_list, columns = c_list)\n",
    "# df = pd.DataFrame(index=r_list, columns = c_list)\n",
    "# print(df)\n",
    "\n",
    "#Creation of resources list from 1st column of dataframe\n",
    "resources = [i.lstrip().rstrip() for i in dfs.iloc[:, 0].tolist()]\n",
    "\n",
    "#Creation of resources_dict which mapping of alphabets to big names from 1st column of dataframe\n",
    "\n",
    "letters = [chr(i) for i in range(65, 88)]\n",
    "resource_dict={}\n",
    "\n",
    "for k,v in zip(letters,resources):\n",
    "    resource_dict[k]=v\n",
    "\n",
    "show_hindi ={}    \n",
    "for k,v in p2w.items():\n",
    "    show_hindi[k] = str(k)+\"_\"+v\n",
    "    \n",
    "show_eng ={}    \n",
    "for k,v in e2w.items():\n",
    "    show_eng[k] = str(k)+\"_\"+v\n",
    "    \n",
    "# print(show_eng)\n",
    "# print(show_hindi)\n",
    "\n",
    "eng = [show_eng[i] for i in sorted(show_eng.keys())]\n",
    "hin = [show_hindi[i] for i in sorted(show_hindi.keys())]\n",
    "\n",
    "title=[\"0\"]+show_eng.values()#.insert(0,'0')\n",
    "\n",
    "\n",
    "eng, hin\n",
    "df = pd.DataFrame(index=hin, columns = eng)\n",
    "df\n",
    "p2w\n",
    "e2w\n",
    "resources\n",
    "resource_dict\n",
    "# resource_dict_invert= {v: k for k, v in resource_dict.items()}\n",
    "print()\n",
    "# dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pandas as pd, numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# tmp_path=os.getenv('HOME_anu_tmp')+'/tmp/'\n",
    "# eng_file_name = 'BUgol2.1E'\n",
    "# eng_file_name = 'BUgol2.2E'\n",
    "# sent_no = '2.25'\n",
    "\n",
    "# path_tmp = tmp_path + eng_file_name + \"_tmp/\" + sent_no\n",
    "# dfs = pd.read_csv(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_1.csv\")\n",
    "# dfs. index = np.arange(1,len(dfs)+1)\n",
    "# print(dfs.shape)\n",
    "\n",
    "\n",
    "# zero_remover = lambda x: x != '0'\n",
    "# hindi_allocations= list(map(zero_remover, dfs.iloc[:,7].tolist()))\n",
    "\n",
    "#dfs.iloc[:,7].tolist()\n",
    "\n",
    "no_of_eng_words = dfs.shape[1]\n",
    "final_row_in_csv =[]; final_row_in_csv1=[]; allocations=[]\n",
    "# display(dfs)\n",
    "for j in range(0,no_of_eng_words):\n",
    "    hindi_allocations_list=[]\n",
    "    #for every row extracted all non zero entries and stored in hindi_allocations_list\n",
    "    hindi_allocations_list = [str(i) for i in dfs.iloc[:,j].tolist() if i!='0' and i!=0]\n",
    "    #print(j, hindi_allocations_list)\n",
    "    \n",
    "    #For an empty hindi_allocations_list i.e verical column equivalent to 1 eng id appended '0' entry in it.\n",
    "    if not hindi_allocations_list:\n",
    "        hindi_allocations_list.append('0')\n",
    "    #print(j, hindi_allocations_list)\n",
    "    \n",
    "    #hindi_allocations string contains all entries in vertical column with \"#\" seperator.\n",
    "    hindi_allocations = \"#\".join([str(i) for i in hindi_allocations_list])\n",
    "        \n",
    "#     if j==0:\n",
    "#         hindi_allocations = \"all\"\n",
    "    final_row_in_csv.append(hindi_allocations)\n",
    "\n",
    "final_row_in_csv[0]=\"Sum of all Resource Suggestions with duplicates\"\n",
    "\n",
    "anchor1=[]; anchor1_str_list=[]; count_dict_list=[]\n",
    "for j in range(0,no_of_eng_words):\n",
    "    temp=[];temp1=[]; temp_count_dict={}\n",
    "\n",
    "#     print(final_row_in_csv[j])\n",
    "#     print(re.split('#|/',final_row_in_csv[j]))\n",
    "\n",
    "    temp = re.split('#|/',final_row_in_csv[j])\n",
    "#     print(temp)\n",
    "    temp1 = list(dict.fromkeys(re.split('#|/',final_row_in_csv[j])))\n",
    "#     print(temp1)\n",
    "#     print([temp.count(x) for x in temp1])\n",
    "    count_info = [temp.count(x) for x in temp1]\n",
    "\n",
    "    for i in range(len(temp1)):\n",
    "        temp_count_dict[temp1[i]] = count_info[i]\n",
    "    \n",
    "    count_dict_list.append(temp_count_dict)\n",
    "#     print(temp_count_dict)\n",
    "    anchor1.append(temp1)\n",
    "    anchor1_str_list.append(\"#\".join(temp1))\n",
    "    \n",
    "    \n",
    "anchor1[0] = 'Sum of all Resource Suggestions without deplucates' \n",
    "anchor1_str_list[0] = 'Sum of all Resource Suggestions without duplicates' \n",
    "# print(anchor1)\n",
    "    \n",
    "dfs.loc[-1] = final_row_in_csv \n",
    "dfs.loc[-2] = anchor1_str_list\n",
    "final_row_in_csv1= anchor1_str_list\n",
    "# dfs.loc[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Roja Transliterate', 'Nandani dict', 'Unique Hindi sugg. entries'], 'Roja Transliterate')\n",
      "(['0', '0', '2'], '2')\n",
      "(['1', '0', '0'], '1')\n",
      "(['0', '0', '10'], '10')\n",
      "(['0', '0', '7'], '7')\n",
      "(['0', '0', '8'], '8')\n",
      "(['0', '0', '9'], '9')\n",
      "(['0', '0', '0'], '0')\n",
      "(['0', '0', '0'], '0')\n",
      "(['0', '0', '3'], '3')\n",
      "(['0', '0', '4 5'], '4 5')\n",
      "('PPPPP', ['final anchor', '2', '1', '10', '7', '8', '9', '0', '0', '3', '4 5'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1_Lake</th>\n",
       "      <th>2_Chilika</th>\n",
       "      <th>3_is</th>\n",
       "      <th>4_an</th>\n",
       "      <th>5_important</th>\n",
       "      <th>6_feature</th>\n",
       "      <th>7_along</th>\n",
       "      <th>8_the</th>\n",
       "      <th>9_eastern</th>\n",
       "      <th>10_coast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>Sum of all Resource Suggestions with duplicates</td>\n",
       "      <td>2#2</td>\n",
       "      <td>0</td>\n",
       "      <td>10#10#10</td>\n",
       "      <td>7#7</td>\n",
       "      <td>8#8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3#3</td>\n",
       "      <td>4 5#4 5#4 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>Sum of all Resource Suggestions without duplic...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-3</th>\n",
       "      <td>Hindi. sug. without conflict entry (cross alig)</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-4</th>\n",
       "      <td>Unique Hindi sugg. entries</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-5</th>\n",
       "      <td>Nandani dict</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-6</th>\n",
       "      <td>Roja Transliterate</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-7</th>\n",
       "      <td>final anchor</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0 1_Lake 2_Chilika  \\\n",
       "-1    Sum of all Resource Suggestions with duplicates    2#2         0   \n",
       "-2  Sum of all Resource Suggestions without duplic...      2         0   \n",
       "-3    Hindi. sug. without conflict entry (cross alig)      2         0   \n",
       "-4                         Unique Hindi sugg. entries      2         0   \n",
       "-5                                       Nandani dict      0         0   \n",
       "-6                                 Roja Transliterate      0         1   \n",
       "-7                                       final anchor      2         1   \n",
       "\n",
       "        3_is 4_an 5_important 6_feature 7_along 8_the 9_eastern     10_coast  \n",
       "-1  10#10#10  7#7         8#8         9       0     0       3#3  4 5#4 5#4 5  \n",
       "-2        10    7           8         9       0     0         3          4 5  \n",
       "-3        10    7           8         9       0     0         3          4 5  \n",
       "-4        10    7           8         9       0     0         3          4 5  \n",
       "-5         0    0           0         0       0     0         0            0  \n",
       "-6         0    0           0         0       0     0         0            0  \n",
       "-7        10    7           8         9       0     0         3          4 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "anchor2=[]; anchor2_str_list=[]\n",
    "for j in range(0,no_of_eng_words):\n",
    "    if len(anchor1[j]) == 1:\n",
    "        temp = anchor1[j]\n",
    "    else:\n",
    "        temp=['0']\n",
    "    anchor2.append(temp)\n",
    "    anchor2_str_list.append(\"\".join(temp))\n",
    "#     print(anchor1[j],\"=>\", anchor2[j])\n",
    "\n",
    "anchor2[0] = 'Hindi sugg. without conflict (cross alignment possibilities)' \n",
    "anchor2_str_list[0] = 'Hindi. sug. without conflict entry (cross alig)' \n",
    "\n",
    "\n",
    "dfs.loc[-3] = anchor2_str_list\n",
    "# dfs.loc[-1:]\n",
    "\n",
    "anchor3_str_list=anchor2_str_list\n",
    "anchor3_str_list[0]=\"Unique Hindi sugg. entries\"\n",
    "repetated_entries=[]\n",
    "for i in range(0, len(anchor2_str_list)):\n",
    "    \n",
    "#     print(anchor2_str_list[i], anchor2_str_list.count(anchor2_str_list[i]))\n",
    "    if anchor2_str_list.count(anchor2_str_list[i]) > 1:\n",
    "        repetated_entries.append(anchor2_str_list[i])\n",
    "        anchor3_str_list[i] = '0'\n",
    "        \n",
    "#     print(anchor2_str_list[i] , anchor3_str_list)\n",
    "#     while anchor2_str_list[i] in anchor3_str_list:\n",
    "#             anchor3_str_list.remove(anchor2_str_list[i])\n",
    "        \n",
    "\n",
    "# print(anchor2_str_list)\n",
    "# print(anchor3_str_list)\n",
    "# print(repetated_entries)\n",
    "for j in range(0, len(anchor3_str_list)):\n",
    "    for i in range(0,len(repetated_entries)):\n",
    "#         print(repetated_entries[i])\n",
    "        if repetated_entries != '0':\n",
    "            if repetated_entries[i] == anchor3_str_list[j]:\n",
    "                anchor3_str_list[j] = '0'\n",
    "          \n",
    "\n",
    "# print(anchor3_str_list)\n",
    "dfs.loc[-4] = anchor3_str_list\n",
    "\n",
    "\n",
    "# print(nandani_mapping)\n",
    "nandani_mapping_list=[]; \n",
    "for j in range(0,no_of_eng_words):\n",
    "    if str(j) in nandani_mapping.keys():\n",
    "#         print(str(j), transliterate_mapping[str(j)])\n",
    "        nandani_mapping_list.append(nandani_mapping[str(j)])\n",
    "    else:\n",
    "#         print(str(j), '0')\n",
    "        nandani_mapping_list.append('0')\n",
    "    \n",
    "    #This is temp module which has nandani's eng_multi to hindi_multi mapping\n",
    "    #Eg. {'10 11': '3 4'} for group of 10 and 11 hindi has 3 and 4.\n",
    "    #TODO FUTYRE: We need to add one more layer showing grouping information \n",
    "    for every_entry in nandani_mapping.keys():\n",
    "        if \" \" in every_entry:\n",
    "            eng_group_list = every_entry.split(\" \")\n",
    "            hindi_group_list = nandani_mapping[every_entry].split(\" \")\n",
    "#             print(eng_group_list, hindi_group_list)\n",
    "            \n",
    "            #Till now nandani's entries are of same length\n",
    "            for i in range(0, len(eng_group_list)):\n",
    "                nandani_mapping[eng_group_list[i]]= hindi_group_list[i]\n",
    "#             print(\"Multiword entry in nandani dictionary\")\n",
    "\n",
    "# print(nandani_mapping)\n",
    "\n",
    "nandani_mapping_list[0] = 'Nandani dict'\n",
    "dfs.loc[-5] = nandani_mapping_list\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    ## print(transliterate_mapping)\n",
    "    roja_transliterate_list=[]; \n",
    "    for j in range(0,no_of_eng_words):\n",
    "        if str(j) in transliterate_mapping.keys():\n",
    "    #         print(str(j), transliterate_mapping[str(j)])\n",
    "            roja_transliterate_list.append(transliterate_mapping[str(j)])\n",
    "        else:\n",
    "    #         print(str(j), '0')\n",
    "            roja_transliterate_list.append('0')\n",
    "    roja_transliterate_list[0] = 'Roja Transliterate'\n",
    "    # print(roja_transliterate_list)   \n",
    "    dfs.loc[-6] = roja_transliterate_list\n",
    "except:\n",
    "    print(\"FILE MISSING: \" + roja_transliterate_file )\n",
    "\n",
    "# try:\n",
    "all_entries=[]; anchor=roja_transliterate_list\n",
    "\n",
    "for i in range(0, no_of_eng_words):\n",
    "    all_entries.append([roja_transliterate_list[i], nandani_mapping_list[i],anchor3_str_list[i]])\n",
    "#     print(all_entries[i])\n",
    "    count_zeroes = all_entries[i].count('0')\n",
    "    if count_zeroes == 3:\n",
    "        anchor[i]='0'\n",
    "    elif count_zeroes == 0:\n",
    "        anchor[i] == 'final anchor'\n",
    "\n",
    "    elif count_zeroes == 2:\n",
    "#         if all_entries[0]!= '0' or all_entries[0]!=0:\n",
    "#             new_entry = all_entries[0]\n",
    "        \n",
    "        new_entry = [z for z in all_entries[i] if z!='0' ]\n",
    "#         for item in all_entries[i]:\n",
    "#             if item !='0' or item!=0:\n",
    "#                 new=item\n",
    "#                 break\n",
    "#             print(all_entries[i] ,\"=>\", new)\n",
    "        anchor[i]= new_entry[0]\n",
    "        \n",
    "    elif count_zeroes ==1:\n",
    "#         print(\"two non zero\")\n",
    "        print(all_entries[i])\n",
    "\n",
    "        \n",
    "#         if all_entries[i][0]!= 'Roja Transliterate' and all_entries[i][1]!= 'Nandani dict' and all_entries[i][2]!= 'Unique Hindi sugg. entries':\n",
    "            \n",
    "        if (all_entries[i][0]!='0' or 0) and (all_entries[i][1]!='0' or 0) and (all_entries[i][2]=='0' or 0) :\n",
    "            print(\"1st to roja\")\n",
    "            anchor[i] = all_entries[i][0]\n",
    "\n",
    "        elif all_entries[i][0]!='0' or 0 and all_entries[i][1]=='0' or 0 and all_entries[i][2]!='0' or 0:\n",
    "            print(\"1st to roja\")\n",
    "            anchor[i] = all_entries[i][0]\n",
    "\n",
    "        elif all_entries[i][0]=='0' and all_entries[i][1]!='0'and all_entries[i][2]!='0' or 0:\n",
    "            print(\"nandani\")\n",
    "            anchor[i]= all_entries[i][1]\n",
    "    print(all_entries[i], anchor[i])   \n",
    "        \n",
    "#     print(count_zeroes)\n",
    "#     print(anchor[i])\n",
    "anchor[0]='final anchor'\n",
    "print(\"PPPPP\",anchor)\n",
    "dfs.loc[-7] = anchor\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "dfs1 = pd.DataFrame(dfs.iloc[-7:,:])\n",
    "dfs1.columns = title\n",
    "# print(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_2.csv\")\n",
    "dfs.to_csv(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_2.csv\", index=False)\n",
    "display(dfs1)\n",
    "# show_hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kishori/a/tmp_anu_dir/tmp/BUgol2.2E_tmp/2.49/BUgol2.2E_2.49_2.csv\n",
      "('PPPPPP', [])\n",
      "*********\n",
      "([], 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1_The</th>\n",
       "      <th>2_Eastern</th>\n",
       "      <th>3_Ghats</th>\n",
       "      <th>4_are</th>\n",
       "      <th>5_discontinuous</th>\n",
       "      <th>6_and</th>\n",
       "      <th>7_irregular</th>\n",
       "      <th>8_and</th>\n",
       "      <th>9_dissected</th>\n",
       "      <th>10_by</th>\n",
       "      <th>11_rivers</th>\n",
       "      <th>12_draining</th>\n",
       "      <th>13_into</th>\n",
       "      <th>14_the</th>\n",
       "      <th>15_Bay</th>\n",
       "      <th>16_of</th>\n",
       "      <th>17_Bengal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-6</th>\n",
       "      <td>Roja Transliterate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0 1_The 2_Eastern 3_Ghats 4_are 5_discontinuous 6_and  \\\n",
       "-6  Roja Transliterate     0         0       0     0               0     0   \n",
       "\n",
       "   7_irregular 8_and 9_dissected 10_by 11_rivers 12_draining 13_into 14_the  \\\n",
       "-6           0     0           0     0         0           0       0      0   \n",
       "\n",
       "   15_Bay 16_of 17_Bengal  \n",
       "-6      0     0        12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5be4d8ad5fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mdfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manchor_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kishori/anaconda2/envs/python3.6/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kishori/anaconda2/envs/python3.6/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                                 raise ValueError(\"cannot set a row with \"\n\u001b[0m\u001b[1;32m    446\u001b[0m                                                  \"mismatched columns\")\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "#OLD CODE\n",
    "\n",
    "# [i for i in dfs.iloc[:,1].tolist() if i!='0' and i!=0]\n",
    "# dfs = pd.read_csv(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_2.csv\")\n",
    "\n",
    "print(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_2.csv\")\n",
    "dfs.to_csv(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_2.csv\", index=False)\n",
    "# dfs.rename(columns = show_eng,inplace=True)\n",
    "\n",
    "\n",
    "last_row = dfs.iloc[-1,:].tolist()\n",
    "last_row_dict={}; temp=[]\n",
    "for i in range(1,len(last_row)):\n",
    "#     print(i, last_row[i])\n",
    "    if \"#\" not in last_row[i]:\n",
    "#         temp.append(last_row[i])\n",
    "#         print([last_row[i]])\n",
    "        last_row_dict[i]=[last_row[i]]\n",
    "    else:\n",
    "#         print(last_row[i])\n",
    "        temp=last_row[i].split(\"#\")\n",
    "#         print(temp)\n",
    "        last_row_dict[i]= last_row[i].split(\"#\")\n",
    "\n",
    "    \n",
    "    \n",
    "last_row_dict[i]= temp\n",
    "# print(\"++++\",temp)\n",
    "\n",
    "dfs1 = pd.DataFrame(dfs.iloc[-1,:]).T\n",
    "# print(show_eng)\n",
    "# print(show_hindi)\n",
    "title=[\"0\"]+show_eng.values()#.insert(0,'0')\n",
    "# print(title)\n",
    "dfs1.columns = title\n",
    "\n",
    "# display(dfs1.iloc[-1:])\n",
    "# p2w\n",
    "final_row_in_csv1\n",
    "hindi_allocations_list\n",
    "allocations\n",
    "merged = (list(itertools.chain.from_iterable(allocations)))\n",
    "print(\"PPPPPP\",merged)\n",
    "anchor_list=[];anchor_tuple_list=[]\n",
    "for i, entry in enumerate(allocations,0):\n",
    "    if entry not in [['0'], ['all_intersect']]:\n",
    "        for index, j in enumerate(entry,0):\n",
    "#             print(\"\"=> i, entry[index], merged.count(entry[index]))\n",
    "            anchor_tuple = (i, entry[index], merged.count(entry[index]))\n",
    "            if anchor_tuple[2]==1:\n",
    "#                 print(i, entry[index], merged.count(entry[index]))\n",
    "                anchor_tuple_list.append((i,entry[index]))\n",
    "print(\"*********\")\n",
    "# print(len(allocations))\n",
    "# print(allocations)\n",
    "anchor_list = [0] * len(allocations)\n",
    "\n",
    "for i in range(len(anchor_tuple_list)):\n",
    "#     print anchor_tuple_list[i]\n",
    "    anchor_list.insert(anchor_tuple_list[i][0], anchor_tuple_list[i][1])\n",
    "anchor_list = anchor_list[0:len(allocations)]\n",
    "print(anchor_list, len(anchor_list))\n",
    "pd.options.display.max_columns = None\n",
    "display(dfs1.iloc[-1:])\n",
    "dfs1.loc[-1]=anchor_list\n",
    "display(dfs1)\n",
    "\n",
    "dfs1.to_csv(path_tmp +'/'+ eng_file_name + \"_\"+sent_no + \"_3.csv\", index=False)\n",
    "show_hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_anchor(l, last_row_dict):\n",
    "#     print(l, len(l))\n",
    "#     print(last_row_dict)\n",
    "#     for i in range(1,len(l)):\n",
    "#         print(i, l[i])\n",
    "\n",
    "       \n",
    "    l1=l\n",
    "    return l1\n",
    " \n",
    "anchored_row = fixing_anchor(last_row, last_row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(allocations)):\n",
    "#     for j in range(len(anchor_tuple_list)):\n",
    "#         if i==anchor_tuple_list[j][0]:\n",
    "#             print(i,anchor_tuple_list[j])\n",
    "            \n",
    "\n",
    "            \n",
    "#             anchor_tuple[i].append(anchor_tuple_list[j][1])\n",
    "# print(anchor_tuple)\n",
    "# for i in range(len(allocations)):\n",
    "#     for j in range(len(anchor_tuple_list)):\n",
    "#         if(anchor_tuple_list[i]==i):\n",
    "#             print(i, allocations[i] , \"<=>\", anchor_tuple_list[j] )\n",
    "# #     print(anchor_tuple_list[i],i)\n",
    "#     if(anchor_tuple_list[i]==i):\n",
    "        \n",
    "#         print(anchor_tuple_list[1])\n",
    "#         anchor_list.append(anchor_tuple_list[1])\n",
    "# anchor_list\n",
    "# dfs1.replace({\"all_intersect\" = p2w})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
